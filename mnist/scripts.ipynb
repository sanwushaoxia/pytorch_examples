{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "\"\"\" 在使用 torch.nn.ReLU 之前, 需要使用 torch.nn.BatchNorm2d 和 torch.nn.BatchNorm1d 将数据分布归一! 否则 torch.nn.ReLU 的效果不如 torch.nn.Sigmoid \"\"\"\n",
    "\n",
    "class LeNet(torch.nn.Module):\n",
    "    def __init__(self, act: torch.nn.Module):\n",
    "        super().__init__() # 执行父类的 __init__() 方法\n",
    "        self.act = act\n",
    "        self.conv1 = torch.nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, padding=(2, 2))\n",
    "        self.bn1 = torch.nn.BatchNorm2d(num_features=6)\n",
    "        self.pool1 = torch.nn.MaxPool2d(kernel_size=2)\n",
    "        self.conv2 = torch.nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5)\n",
    "        self.bn2 = torch.nn.BatchNorm2d(num_features=16)\n",
    "        self.pool2 = torch.nn.MaxPool2d(kernel_size=2)\n",
    "        self.fc1 = torch.nn.Linear(in_features=400, out_features=120)\n",
    "        self.bn3 = torch.nn.BatchNorm1d(num_features=120)\n",
    "        self.fc2 = torch.nn.Linear(in_features=120, out_features=84)\n",
    "        self.bn4 = torch.nn.BatchNorm1d(num_features=84)\n",
    "        self.fc3 = torch.nn.Linear(in_features=84, out_features=10)\n",
    "        self.bn5 = torch.nn.BatchNorm1d(num_features=10)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.conv1(x)           # torch.Size([100, 6, 28, 28])\n",
    "        x = self.bn1(x)             # torch.Size([100, 6, 28, 28])\n",
    "        x = self.act(x)             # torch.Size([100, 6, 28, 28])\n",
    "        x = self.pool1(x)           # torch.Size([100, 6, 14, 14])\n",
    "        x = self.conv2(x)           # torch.Size([100, 16, 10, 10])\n",
    "        x = self.bn2(x)             # torch.Size([100, 16, 10, 10])\n",
    "        x = self.act(x)             # torch.Size([100, 16, 10, 10])\n",
    "        x = self.pool2(x)           # torch.Size([100, 16, 5, 5])\n",
    "        x = x.view(x.size(0), -1)   # torch.Size([100, 400])\n",
    "        x = self.fc1(x)             # torch.Size([100, 120])\n",
    "        x = self.bn3(x)             # torch.Size([100, 120])\n",
    "        x = self.act(x)             # torch.Size([100, 120])\n",
    "        x = self.fc2(x)             # torch.Size([100, 84])\n",
    "        x = self.bn4(x)             # torch.Size([100, 84])\n",
    "        x = self.act(x)             # torch.Size([100, 84])\n",
    "        x = self.fc3(x)             # torch.Size([100, 10])\n",
    "        x = self.bn5(x)             # torch.Size([100, 10])\n",
    "        x = self.act(x)             # torch.Size([100, 10])\n",
    "        # x = torch.nn.functional.log_softmax(x, dim=1) # torch.Size([100, 10])\n",
    "\n",
    "        return x\n",
    "\n",
    "def get_data_loader(train: bool = True, batch_size: int = 1):\n",
    "    return torch.utils.data.DataLoader(\n",
    "        dataset=torchvision.datasets.MNIST(\n",
    "            \"code/python/datasets\",\n",
    "            train=train,\n",
    "            transform=v2.Compose([\n",
    "                v2.ToImage(),\n",
    "                v2.ToDtype(torch.float32, scale=True)\n",
    "            ]),\n",
    "            download=True\n",
    "        ),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "def evaluate(test_data, net: torch.nn.Module):\n",
    "    n_correct = 0\n",
    "    n_total = 0\n",
    "    with torch.no_grad(): # 不计算梯度\n",
    "        for (x, y) in test_data:\n",
    "            outputs = net.forward(x)\n",
    "            for i, output in enumerate(outputs): # torch.Size([10])\n",
    "                # print(output.size())\n",
    "                if torch.argmax(output) == y[i]:\n",
    "                    n_correct += 1\n",
    "                n_total += 1\n",
    "    return n_correct / n_total\n",
    "\n",
    "def main():\n",
    "    train_data = get_data_loader(train=True, batch_size=100)\n",
    "    test_data = get_data_loader(train=False, batch_size=100)\n",
    "    net = LeNet(act=torch.nn.ReLU())\n",
    "    # net = LeNet(act=torch.nn.Sigmoid())\n",
    "\n",
    "    print(\"initial accuracy:\", evaluate(test_data, net))\n",
    "\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
    "    for epoch in range(3):\n",
    "        t0 = time.perf_counter()\n",
    "        for (x, y) in train_data:\n",
    "            net.zero_grad() # 清空梯度\n",
    "            output = net.forward(x) # 前向传播\n",
    "            loss = torch.nn.CrossEntropyLoss()(output, y) # 等价于 nll_loss(log_softmax)\n",
    "            # loss = torch.nn.functional.nll_loss(output, y) # 计算损失\n",
    "            loss.backward() # 反向传播\n",
    "            optimizer.step() # 优化参数\n",
    "        t1 = time.perf_counter()\n",
    "        print(\"epoch \", epoch, \"accuracy: \", evaluate(test_data, net), \"time: \", t1 - t0)\n",
    "\n",
    "    # for (n, (x, _)) in enumerate(test_data):\n",
    "    #     if n > 3:\n",
    "    #         break\n",
    "    #     predict = torch.argmax(net.forward(x[0].view(-1, 28*28)))\n",
    "    #     plt.figure(n)\n",
    "    #     plt.imshow(x[0].view(28, 28))\n",
    "    #     plt.title(\"prediction: \" + str(int(predict)))\n",
    "    # plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    # test_data = get_data_loader(train=False)\n",
    "    # evaluate(test_data, LeNet())\n",
    "    # for (x, y) in test_data:\n",
    "    #     # print(x.size(), y.size())\n",
    "    #     # print(x[0].size())\n",
    "    #     x = LeNet().forward(x)\n",
    "    #     break\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
